{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento de warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r datatran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datatran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise da causa do acidente, coluna de texto atrav√©s de Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aus√™ncia de rea√ß√£o do condutor', 'Entrada inopinada do pedestre',\n",
       "       'Rea√ß√£o tardia ou ineficiente do condutor',\n",
       "       'Velocidade Incompat√≠vel', 'Acumulo de √°gua sobre o pavimento',\n",
       "       'Condutor Dormindo', 'Desrespeitar a prefer√™ncia no cruzamento',\n",
       "       'Demais falhas mec√¢nicas ou el√©tricas', 'Transitar na contram√£o',\n",
       "       'Acessar a via sem observar a presen√ßa dos outros ve√≠culos',\n",
       "       'Manobra de mudan√ßa de faixa',\n",
       "       'Avarias e/ou desgaste excessivo no pneu', 'Pista Escorregadia',\n",
       "       'Condutor deixou de manter dist√¢ncia do ve√≠culo da frente',\n",
       "       'Trafegar com motocicleta (ou similar) entre as faixas',\n",
       "       'Pista esburacada', 'Pedestre andava na pista',\n",
       "       'Acumulo de √≥leo sobre o pavimento',\n",
       "       'Carga excessiva e/ou mal acondicionada',\n",
       "       'Condutor desrespeitou a ilumina√ß√£o vermelha do sem√°foro', 'Chuva',\n",
       "       'Mal s√∫bito do condutor', 'Curva acentuada', 'Obstru√ß√£o na via',\n",
       "       'Ultrapassagem Indevida', 'Afundamento ou ondula√ß√£o no pavimento',\n",
       "       'Frear bruscamente', 'Problema com o freio',\n",
       "       'Demais falhas na via', 'Animais na Pista',\n",
       "       'Problema na suspens√£o', 'Ingest√£o de √°lcool pelo condutor',\n",
       "       'Convers√£o proibida', 'Pedestre cruzava a pista fora da faixa',\n",
       "       'Restri√ß√£o de visibilidade em curvas horizontais',\n",
       "       'Acostamento em desn√≠vel',\n",
       "       'Falta de elemento de conten√ß√£o que evite a sa√≠da do leito carro√ß√°vel',\n",
       "       'Estacionar ou parar em local proibido', 'Aus√™ncia de sinaliza√ß√£o',\n",
       "       'Obras na pista', 'Acesso irregular', 'Desvio tempor√°rio',\n",
       "       'Ingest√£o de √°lcool e/ou subst√¢ncias psicoativas pelo pedestre',\n",
       "       'Ingest√£o de subst√¢ncias psicoativas pelo condutor',\n",
       "       'Retorno proibido', 'Condutor usando celular', 'Neblina',\n",
       "       'Falta de acostamento', 'Redutor de velocidade em desacordo',\n",
       "       'Transitar na cal√ßada',\n",
       "       'Acumulo de areia ou detritos sobre o pavimento',\n",
       "       'Modifica√ß√£o proibida', 'Sinaliza√ß√£o mal posicionada',\n",
       "       'Pista em desn√≠vel', 'Objeto est√°tico sobre o leito carro√ß√°vel',\n",
       "       'Ilumina√ß√£o deficiente',\n",
       "       'Ingest√£o de √°lcool ou de subst√¢ncias psicoativas pelo pedestre',\n",
       "       'Defici√™ncia do Sistema de Ilumina√ß√£o/Sinaliza√ß√£o',\n",
       "       'Demais Fen√¥menos da natureza', 'Declive acentuado',\n",
       "       'Obstru√ß√£o Via tentativa Assalto',\n",
       "       'Deixar de acionar o farol da motocicleta (ou similar)', 'Fuma√ßa',\n",
       "       'Faixas de tr√¢nsito com largura insuficiente',\n",
       "       'Sinaliza√ß√£o encoberta', 'Transitar no Acostamento',\n",
       "       'Restri√ß√£o de visibilidade em curvas verticais',\n",
       "       '√Årea urbana sem a presen√ßa de local apropriado para a travessia de pedestres',\n",
       "       'Suic√≠dio (presumido)', 'Transtornos Mentais (exceto suicidio)',\n",
       "       'Sistema de drenagem ineficiente', 'Participar de racha',\n",
       "       'Far√≥is desregulados',\n",
       "       'Pedestre - Ingest√£o de √°lcool/ subst√¢ncias psicoativas',\n",
       "       'Sem√°foro com defeito'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['causa_acidente'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40481143-703f-4ca3-8f35-edfdfba63f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tokeniza√ß√£o para remo√ß√£o de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baixar dados necess√°rios do NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(stopwords.words('portuguese')) | set([\n",
    "    \" \", \"ou\", \"\", \"ok\", \"problema\", \"paciente\", \"necess√°rio\", \"feito\", \"w\", \"devido\",\n",
    "    \"precisa\", \"dr\", \"op√ß√µes\", \"caso\", \"o\", \"x\", \"tentar\", \"ainda\", \"pr√≥ximo\", \"r\",\n",
    "    \"d\", \"desde\", \"teste\", \"testando\", \"resultados\", \"recomendar\", \"pode\", \"por favor\",\n",
    "    \"m√©dico\", \"normal\", \"seria\", \"discutido\", \"medicamento\", \"doen√ßa\",\n",
    "    \"sugerido\", \"considerar\", \"sim\", \"prov√°vel\", \"cl√≠nico\", \"revis√£o\",\n",
    "    \"interno\", \"tratar\", \"rever\", \"semana\", \"ensaio\", \"coment√°rio\", \"rec\",\n",
    "    \"vai\", \"n√≥s\", \"oi\", \"ol√°\", \"cumprimento\", \"gostar\", \"saber\", \"sim\", \"certo\",\n",
    "    \"amanh√£\", \"olhar\", \"dizer\", \"okay\", \"quilograma\", \"zoom\", \"link\", \"tudo bem\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar e remover stopwords\n",
    "def process_text(text):\n",
    "        tokens = word_tokenize(str(text).lower())  # Tokenizar e converter para min√∫sculas\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in custom_stopwords]\n",
    "        return \" \".join(filtered_tokens)  # Reunir palavras filtradas em uma string\n",
    "\n",
    "# Aplicar processamento ao DataFrame\n",
    "df[\"palavras_filtradas\"] = df[\"causa_acidente\"].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ano</th>\n",
       "      <th>feriado</th>\n",
       "      <th>mes</th>\n",
       "      <th>data_inversa</th>\n",
       "      <th>uf</th>\n",
       "      <th>br</th>\n",
       "      <th>km</th>\n",
       "      <th>municipio</th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>...</th>\n",
       "      <th>fase_dia</th>\n",
       "      <th>dia_semana</th>\n",
       "      <th>tipo_acidente</th>\n",
       "      <th>tipo_pista</th>\n",
       "      <th>mortos</th>\n",
       "      <th>feridos_graves</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>palavras_filtradas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496519.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>ES</td>\n",
       "      <td>101</td>\n",
       "      <td>114</td>\n",
       "      <td>SOORETAMA</td>\n",
       "      <td>Aus√™ncia de rea√ß√£o do condutor</td>\n",
       "      <td>...</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>domingo</td>\n",
       "      <td>Sa√≠da de leito carro√ß√°vel</td>\n",
       "      <td>Simples</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-19,09484877</td>\n",
       "      <td>-40,05095848</td>\n",
       "      <td>Segura</td>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496543.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>SP</td>\n",
       "      <td>116</td>\n",
       "      <td>113,1</td>\n",
       "      <td>TAUBATE</td>\n",
       "      <td>Entrada inopinada do pedestre</td>\n",
       "      <td>...</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>domingo</td>\n",
       "      <td>Atropelamento de Pedestre</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-23,0445658</td>\n",
       "      <td>-45,58259814</td>\n",
       "      <td>Perigosa</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   ano feriado  mes data_inversa  uf   br     km  municipio  \\\n",
       "0  496519.0  2023       y    1   2023-01-01  ES  101    114  SOORETAMA   \n",
       "1  496543.0  2023       y    1   2023-01-01  SP  116  113,1    TAUBATE   \n",
       "\n",
       "                   causa_acidente  ...     fase_dia  dia_semana  \\\n",
       "0  Aus√™ncia de rea√ß√£o do condutor  ...  Plena Noite     domingo   \n",
       "1   Entrada inopinada do pedestre  ...  Plena Noite     domingo   \n",
       "\n",
       "               tipo_acidente tipo_pista mortos feridos_graves      latitude  \\\n",
       "0  Sa√≠da de leito carro√ß√°vel    Simples      0              0  -19,09484877   \n",
       "1  Atropelamento de Pedestre      Dupla      1              0   -23,0445658   \n",
       "\n",
       "      longitude    status          palavras_filtradas  \n",
       "0  -40,05095848    Segura    aus√™ncia rea√ß√£o condutor  \n",
       "1  -45,58259814  Perigosa  entrada inopinada pedestre  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematiza√ß√£o e Radicaliza√ß√£o\n",
    "\n",
    "üîπ Lema (Lemmatization): Processo de reduzir palavras √† sua forma base ou raiz, conhecida como lema.\n",
    "\n",
    "üîπ Radical (Stemming): Redu√ß√£o de palavras √† sua forma raiz (stem) sem considerar regras gramaticais ou significado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def lemma_nltk(df):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def lemmatize_words(text):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    # Apply the lemmatization function to the DataFrame column\n",
    "    df['lemma'] = df['palavras_filtradas'].apply(lemmatize_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_lemma = lemma_nltk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rea√ß√£o tardia ineficiente condutor</td>\n",
       "      <td>rea√ß√£o tardia ineficiente condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acumulo √°gua sobre pavimento</td>\n",
       "      <td>acumulo √°gua sobre pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desrespeitar prefer√™ncia cruzamento</td>\n",
       "      <td>desrespeitar prefer√™ncia cruzamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demais falhas mec√¢nicas el√©tricas</td>\n",
       "      <td>demais falhas mec√¢nicas el√©tricas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transitar contram√£o</td>\n",
       "      <td>transitar contram√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    palavras_filtradas                                lemma\n",
       "0             aus√™ncia rea√ß√£o condutor             aus√™ncia rea√ß√£o condutor\n",
       "1           entrada inopinada pedestre           entrada inopinada pedestre\n",
       "2   rea√ß√£o tardia ineficiente condutor   rea√ß√£o tardia ineficiente condutor\n",
       "3              velocidade incompat√≠vel              velocidade incompat√≠vel\n",
       "4         acumulo √°gua sobre pavimento         acumulo √°gua sobre pavimento\n",
       "5                    condutor dormindo                    condutor dormindo\n",
       "6  desrespeitar prefer√™ncia cruzamento  desrespeitar prefer√™ncia cruzamento\n",
       "7    demais falhas mec√¢nicas el√©tricas    demais falhas mec√¢nicas el√©tricas\n",
       "8                  transitar contram√£o                  transitar contram√£o\n",
       "9              velocidade incompat√≠vel              velocidade incompat√≠vel"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemma[['palavras_filtradas', 'lemma']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_nltk_after_lemma(df):\n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def stem_words(text):\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "        # Stem each word\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        # Join the stemmed words back into a single string\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    # Apply the stemming function to the DataFrame\n",
    "    df['normalizado'] = df['lemma'].apply(stem_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_final = stem_nltk_after_lemma(df_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rea√ß√£o tardia ineficiente condutor</td>\n",
       "      <td>rea√ß√£o tardia ineficiente condutor</td>\n",
       "      <td>rea√ß√£o tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidad incompat√≠vel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acumulo √°gua sobre pavimento</td>\n",
       "      <td>acumulo √°gua sobre pavimento</td>\n",
       "      <td>acumulo √°gua sobr pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desrespeitar prefer√™ncia cruzamento</td>\n",
       "      <td>desrespeitar prefer√™ncia cruzamento</td>\n",
       "      <td>desrespeitar prefer√™ncia cruzamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demais falhas mec√¢nicas el√©tricas</td>\n",
       "      <td>demais falhas mec√¢nicas el√©tricas</td>\n",
       "      <td>demai falha mec√¢nica el√©trica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transitar contram√£o</td>\n",
       "      <td>transitar contram√£o</td>\n",
       "      <td>transitar contram√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidade incompat√≠vel</td>\n",
       "      <td>velocidad incompat√≠vel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    palavras_filtradas                                lemma  \\\n",
       "0             aus√™ncia rea√ß√£o condutor             aus√™ncia rea√ß√£o condutor   \n",
       "1           entrada inopinada pedestre           entrada inopinada pedestre   \n",
       "2   rea√ß√£o tardia ineficiente condutor   rea√ß√£o tardia ineficiente condutor   \n",
       "3              velocidade incompat√≠vel              velocidade incompat√≠vel   \n",
       "4         acumulo √°gua sobre pavimento         acumulo √°gua sobre pavimento   \n",
       "5                    condutor dormindo                    condutor dormindo   \n",
       "6  desrespeitar prefer√™ncia cruzamento  desrespeitar prefer√™ncia cruzamento   \n",
       "7    demais falhas mec√¢nicas el√©tricas    demais falhas mec√¢nicas el√©tricas   \n",
       "8                  transitar contram√£o                  transitar contram√£o   \n",
       "9              velocidade incompat√≠vel              velocidade incompat√≠vel   \n",
       "\n",
       "                           normalizado  \n",
       "0             aus√™ncia rea√ß√£o condutor  \n",
       "1            entrada inopinada pedestr  \n",
       "2    rea√ß√£o tardia ineficient condutor  \n",
       "3               velocidad incompat√≠vel  \n",
       "4          acumulo √°gua sobr pavimento  \n",
       "5                    condutor dormindo  \n",
       "6  desrespeitar prefer√™ncia cruzamento  \n",
       "7        demai falha mec√¢nica el√©trica  \n",
       "8                  transitar contram√£o  \n",
       "9               velocidad incompat√≠vel  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['palavras_filtradas', 'lemma', 'normalizado']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ano</th>\n",
       "      <th>feriado</th>\n",
       "      <th>mes</th>\n",
       "      <th>data_inversa</th>\n",
       "      <th>uf</th>\n",
       "      <th>br</th>\n",
       "      <th>km</th>\n",
       "      <th>municipio</th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>...</th>\n",
       "      <th>tipo_acidente</th>\n",
       "      <th>tipo_pista</th>\n",
       "      <th>mortos</th>\n",
       "      <th>feridos_graves</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, ano, feriado, mes, data_inversa, uf, br, km, municipio, causa_acidente, classificacao_acidente, veiculos, condicao_metereologica, fase_dia, dia_semana, tipo_acidente, tipo_pista, mortos, feridos_graves, latitude, longitude, status, palavras_filtradas, lemma, normalizado]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['lemma']==''].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (entendendo o algoritmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lda(df_final, topics, words, threshold):\n",
    "    \"\"\"\n",
    "    Executa a **Latent Dirichlet Allocation (LDA)** em um DataFrame para identificar t√≥picos.\n",
    "\n",
    "    **Par√¢metros:**\n",
    "    df_final (DataFrame): O DataFrame de entrada contendo os dados de texto.\n",
    "    topics (int): O n√∫mero de t√≥picos a identificar.\n",
    "    words (int): O n√∫mero de palavras mais relevantes a serem exibidas para cada t√≥pico.\n",
    "\n",
    "    **Retorna:**\n",
    "    df_topics (DataFrame): O DataFrame de sa√≠da contendo os dados de texto.\n",
    "\n",
    "    Esta fun√ß√£o divide o DataFrame em conjuntos de treino e teste, **vetoriza os dados de texto** usando **TF-IDF**, \n",
    "    ajusta um modelo LDA nos dados de treino e transforma os dados de teste. \n",
    "    Ela imprime o n√∫mero de linhas nos conjuntos de treino e teste, confirma a cria√ß√£o das matrizes correspondentes, \n",
    "    e indica a conclus√£o das fases de treinamento e teste.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar o DataFrame em conjuntos de treino e teste\n",
    "    train_df = df_final.sample(frac=0.7, random_state=42)\n",
    "    test_df = df_final.drop(train_df.index)\n",
    "\n",
    "    print(f\"Contagem df treino: {len(train_df)}\")\n",
    "    print(f\"Contagem df teste: {len(test_df)}\")\n",
    "\n",
    "    train_texts = train_df['normalizado'].tolist()\n",
    "    test_texts = test_df['normalizado'].tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_matrix = vectorizer.fit_transform(train_texts)\n",
    "    test_matrix = vectorizer.transform(test_texts)\n",
    "\n",
    "    print(f\"Matrix de treino e testes criadas!\")\n",
    "\n",
    "    # Define o n√∫mero de t√≥picos e palavras dentro de cada t√≥pico\n",
    "    num_topics = topics\n",
    "    num_top_words = words\n",
    "\n",
    "    # Cria√ß√£o LDA object\n",
    "    lda = LatentDirichletAllocation(doc_topic_prior=0.5, learning_decay=0.5, learning_method='online', max_iter= 10, topic_word_prior=0.1, n_components=num_topics, random_state=42)\n",
    "\n",
    "    # Fit do modelo na matrix de treino\n",
    "    lda_matrix_train = lda.fit_transform(train_matrix)\n",
    "\n",
    "    print(f\"Treinamento completo!\")\n",
    "\n",
    "    # Transforma a matrix de teste\n",
    "    lda_matrix_test = lda.transform(test_matrix)\n",
    "\n",
    "    print(f\"Teste completo!\")\n",
    "\n",
    "    # Obt√©m os termos de cada t√≥pico\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    topics = []\n",
    "    top_terms_dict = {}\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        top_terms = [terms[i] for i in topic.argsort()[-num_top_words:]]\n",
    "        topics.append((f\"Topic {idx + 1}\", \", \".join(top_terms)))\n",
    "        top_terms_dict[idx] = \", \".join(top_terms)\n",
    "\n",
    "    # Cria√ß√£o df com os t√≥picos\n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\", \"Top Terms\"])\n",
    "\n",
    "    print(f\"T√≥picos gerados!\")\n",
    "\n",
    "    # Teste de perplexidade\n",
    "    perplexity = lda.perplexity(test_matrix)\n",
    "    print(f'Perplexity: {perplexity}')\n",
    " \n",
    "    # Prepara o dado para coherence model\n",
    "    texts = [doc.split() for doc in test_texts]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Fit do modelo LDA usando gensim\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "    # C√°lculo coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "    # Atribue os t√≥picos para o df original\n",
    "    # O par√¢metro de limite (threshold) √© utilizado para garantir que apenas t√≥picos com uma probabilidade acima do limite especificado sejam atribu√≠dos. Se a maior probabilidade estiver abaixo desse limite, o t√≥pico √© definido como -1, indicando que n√£o h√° uma atribui√ß√£o clara de t√≥pico. Essa abordagem pode ajudar a melhorar a precis√£o do mapeamento de t√≥picos.\n",
    "    num_threshold = threshold\n",
    "    topic_prob_matrix = lda.transform(vectorizer.transform(df_final['normalizado'].tolist()))\n",
    "    df_final['Topic'] = np.where(topic_prob_matrix.max(axis=1) >= num_threshold, topic_prob_matrix.argmax(axis=1), -1)\n",
    "    df_final['Top Terms'] = df_final['Topic'].map(lambda x: top_terms_dict.get(x, ''))\n",
    "\n",
    "    return df_topics, df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem df treino: 114395\n",
      "Contagem df teste: 49026\n",
      "Matrix de treino e testes criadas!\n",
      "Treinamento completo!\n",
      "Teste completo!\n",
      "T√≥picos gerados!\n",
      "Perplexity: 44.78924551228605\n",
      "Coherence Score: 0.5328602001293569\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_topics, df_final = lda(df_final, 8, 4, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>contram√£o, transitar, incompat√≠vel, velocidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pedestr, animai, chuva, pista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>dormindo, √°lcool, ingest√£o, condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>observar, presen√ßa, via, ve√≠culo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>acostamento, acumulo, pavimento, sobr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>mec√¢nica, el√©trica, falha, demai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>frear, manobra, mudan√ßa, faixa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic                                      Top Terms\n",
       "3       0  contram√£o, transitar, incompat√≠vel, velocidad\n",
       "1       1                  pedestr, animai, chuva, pista\n",
       "5       2           dormindo, √°lcool, ingest√£o, condutor\n",
       "11      3               observar, presen√ßa, via, ve√≠culo\n",
       "0       4         ineficient, aus√™ncia, condutor, rea√ß√£o\n",
       "4       5          acostamento, acumulo, pavimento, sobr\n",
       "7       6               mec√¢nica, el√©trica, falha, demai\n",
       "17      7                 frear, manobra, mudan√ßa, faixa"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Topic', 'Top Terms']].drop_duplicates().sort_values(by='Topic', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_para_categoria = {\n",
    "    0: \"Infra√ß√£o de Tr√¢nsito\",\n",
    "    1: \"Obst√°culo na pista ou pedestre\",\n",
    "    2: \"Condutor sob efeitos de subst√¢ncias\",\n",
    "    3: \"Manobra imprudente condutor\",\n",
    "    4: \"Manobra imprudente condutor\",\n",
    "    5: \"Acostamento\",\n",
    "    6: \"Problema mec√¢nico no ve√≠culo\",\n",
    "    7: \"Manobra imprudente condutor\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorando os t√≥picos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Acessar a via sem observar a presen√ßa dos outros ve√≠culos</td>\n",
       "      <td>3</td>\n",
       "      <td>observar, presen√ßa, via, ve√≠culo</td>\n",
       "      <td>acessar via observar presen√ßa outro ve√≠culo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               causa_acidente  Topic  \\\n",
       "11  Acessar a via sem observar a presen√ßa dos outros ve√≠culos      3   \n",
       "\n",
       "                           Top Terms  \\\n",
       "11  observar, presen√ßa, via, ve√≠culo   \n",
       "\n",
       "                                    normalizado  \n",
       "11  acessar via observar presen√ßa outro ve√≠culo  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['normalizado'].str.contains('observar', na=False)][['causa_acidente','Topic', 'Top Terms', 'normalizado']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "      <td>rea√ß√£o tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>7</td>\n",
       "      <td>frear, manobra, mudan√ßa, faixa</td>\n",
       "      <td>sistema drenagem ineficient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic                               Top Terms  \\\n",
       "2          4  ineficient, aus√™ncia, condutor, rea√ß√£o   \n",
       "12502      7          frear, manobra, mudan√ßa, faixa   \n",
       "\n",
       "                             normalizado  \n",
       "2      rea√ß√£o tardia ineficient condutor  \n",
       "12502        sistema drenagem ineficient  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['normalizado'].str.contains('ineficient', na=False)][['Topic', 'Top Terms', 'normalizado']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando Medidas de Acur√°cia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perplexity\n",
    "A perplexidade √© uma medida de qu√£o bem um modelo probabil√≠stico prev√™ um conjunto de amostras. No contexto de LDA (Latent Dirichlet Allocation), valores mais baixos de perplexidade indicam um melhor ajuste aos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Coherence Score\n",
    "A coer√™ncia mede a similaridade sem√¢ntica entre as palavras mais relevantes dentro dos t√≥picos. Pontua√ß√µes mais altas indicam t√≥picos de melhor qualidade. \n",
    "\n",
    "Um Coherence Score de 0.5329 indica que os t√≥picos gerados pelo modelo LDA possuem um n√≠vel razo√°vel de interpretabilidade e similaridade sem√¢ntica.\n",
    "Interpreta√ß√£o da pontua√ß√£o:\n",
    "\n",
    "- Acima de 0.5 ‚Üí Indica que os t√≥picos t√™m uma coer√™ncia moderada a boa, com palavras que fazem sentido juntas.\n",
    "- Entre 0.6 e 0.8 ‚Üí Indica boa coer√™ncia sem√¢ntica, sugerindo que os t√≥picos est√£o bem agrupados e interpret√°veis.\n",
    "- Acima de 0.8 ‚Üí Indica alta qualidade, onde os t√≥picos gerados s√£o claramente diferenci√°veis e t√™m forte significado sem√¢ntico.\n",
    "\n",
    "üîπ Resultado (0.5329) sugere que os t√≥picos fazem sentido, mas ainda h√° espa√ßo para refinamento. Algumas melhorias que podem ser feitas para aumentar a coer√™ncia:\n",
    "\n",
    "- Refinar o pr√©-processamento do texto ‚Üí Remover palavras irrelevantes ou normalizar melhor o texto.\n",
    "- Ajustar o n√∫mero de t√≥picos ‚Üí Talvez reduzir ou aumentar um pouco os t√≥picos ajude na segmenta√ß√£o das palavras.\n",
    "- Alterar hiperpar√¢metros do LDA ‚Üí Testar ajustes na alpha e beta, al√©m de aumentar o n√∫mero de itera√ß√µes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID SEARCH \n",
    "\n",
    ">- simplificado devido √† problemas de performance do micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grid_search import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem df treino: 114395\n",
      "Contagem df teste: 49026\n",
      "Matrix de treino e testes criadas!\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.2s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  41.8s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  27.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  30.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  29.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  30.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  30.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  46.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  47.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  47.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  28.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  42.8s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.3min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.3min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.4min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  60.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  44.1s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  43.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  44.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  44.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  45.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  44.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  44.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  46.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  46.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  45.2s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  44.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  59.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  59.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  59.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  45.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  57.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  57.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  58.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  59.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  57.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  57.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  57.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  55.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  40.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.7s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.7s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  39.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  38.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  39.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.1s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  38.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  38.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  56.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  56.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  58.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  57.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  57.2s\n",
      "Best Parameters: {'doc_topic_prior': 0.1, 'learning_decay': 0.3, 'learning_method': 'batch', 'max_iter': 10, 'n_components': 5, 'topic_word_prior': 0.01}\n",
      "Teste completo!\n",
      "T√≥picos gerados!\n",
      "Perplexity: 34.17775315164295\n",
      "Coherence Score: 0.5742541198565523\n",
      "Wall time: 1h 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_topics, df_final = grid_search(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>cruzamento, desrespeitar, prefer√™ncia, dormindo, condutor, faixa, manobra, mudan√ßa, √°lcool, ingest√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  \\\n",
       "3       0   \n",
       "1       1   \n",
       "5       2   \n",
       "11      3   \n",
       "0       4   \n",
       "\n",
       "                                                                                                           Top Terms  \n",
       "3   proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad  \n",
       "1                                      avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai  \n",
       "5               cruzamento, desrespeitar, prefer√™ncia, dormindo, condutor, faixa, manobra, mudan√ßa, √°lcool, ingest√£o  \n",
       "11                                frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo  \n",
       "0                     fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Topic', 'Top Terms']].drop_duplicates().sort_values(by='Topic', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_para_categoria = {\n",
    "    0: \"Infra√ß√£o de Tr√¢nsito\",\n",
    "    1: \"Problema mec√¢nico no ve√≠culo\",\n",
    "    2: \"Condutor sob efeitos de subst√¢ncias\",\n",
    "    3: \"Manobra imprudente condutor\",\n",
    "    4: \"Manobra imprudente condutor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Categoria'] = df_final['Topic'].map(topico_para_categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "      <td>aus√™ncia rea√ß√£o condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Problema mec√¢nico no ve√≠culo</td>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai</td>\n",
       "      <td>entrada inopinada pedestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "      <td>rea√ß√£o tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infra√ß√£o de Tr√¢nsito</td>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad</td>\n",
       "      <td>velocidad incompat√≠vel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Problema mec√¢nico no ve√≠culo</td>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai</td>\n",
       "      <td>acumulo √°gua sobr pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163416</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o</td>\n",
       "      <td>rea√ß√£o tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163417</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>3</td>\n",
       "      <td>frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo</td>\n",
       "      <td>acessar via observar presen√ßa outro ve√≠culo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163418</th>\n",
       "      <td>Infra√ß√£o de Tr√¢nsito</td>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad</td>\n",
       "      <td>transitar contram√£o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163419</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>3</td>\n",
       "      <td>frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo</td>\n",
       "      <td>condutor deixou manter dist√¢ncia ve√≠culo frent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163420</th>\n",
       "      <td>Condutor sob efeitos de subst√¢ncias</td>\n",
       "      <td>2</td>\n",
       "      <td>cruzamento, desrespeitar, prefer√™ncia, dormindo, condutor, faixa, manobra, mudan√ßa, √°lcool, ingest√£o</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163421 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Categoria  Topic  \\\n",
       "0               Manobra imprudente condutor      4   \n",
       "1              Problema mec√¢nico no ve√≠culo      1   \n",
       "2               Manobra imprudente condutor      4   \n",
       "3                      Infra√ß√£o de Tr√¢nsito      0   \n",
       "4              Problema mec√¢nico no ve√≠culo      1   \n",
       "...                                     ...    ...   \n",
       "163416          Manobra imprudente condutor      4   \n",
       "163417          Manobra imprudente condutor      3   \n",
       "163418                 Infra√ß√£o de Tr√¢nsito      0   \n",
       "163419          Manobra imprudente condutor      3   \n",
       "163420  Condutor sob efeitos de subst√¢ncias      2   \n",
       "\n",
       "                                                                                                               Top Terms  \\\n",
       "0                         fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o   \n",
       "1                                          avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai   \n",
       "2                         fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o   \n",
       "3       proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad   \n",
       "4                                          avaria, pneu, pedestr, animai, chuva, pista, el√©trica, mec√¢nica, falha, demai   \n",
       "...                                                                                                                  ...   \n",
       "163416                    fen√¥meno, usando, celular, suic√≠dio, presumido, tardia, ineficient, aus√™ncia, condutor, rea√ß√£o   \n",
       "163417                                frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo   \n",
       "163418  proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contram√£o, incompat√≠vel, velocidad   \n",
       "163419                                frent, dist√¢ncia, manter, deixou, via, presen√ßa, acessar, observar, outro, ve√≠culo   \n",
       "163420              cruzamento, desrespeitar, prefer√™ncia, dormindo, condutor, faixa, manobra, mudan√ßa, √°lcool, ingest√£o   \n",
       "\n",
       "                                           normalizado  \n",
       "0                             aus√™ncia rea√ß√£o condutor  \n",
       "1                            entrada inopinada pedestr  \n",
       "2                    rea√ß√£o tardia ineficient condutor  \n",
       "3                               velocidad incompat√≠vel  \n",
       "4                          acumulo √°gua sobr pavimento  \n",
       "...                                                ...  \n",
       "163416               rea√ß√£o tardia ineficient condutor  \n",
       "163417     acessar via observar presen√ßa outro ve√≠culo  \n",
       "163418                             transitar contram√£o  \n",
       "163419  condutor deixou manter dist√¢ncia ve√≠culo frent  \n",
       "163420                               condutor dormindo  \n",
       "\n",
       "[163421 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Categoria', 'Topic', 'Top Terms', 'normalizado']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando para o PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('./resultados/Data categorizado.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "consult_nlp_vivi",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
